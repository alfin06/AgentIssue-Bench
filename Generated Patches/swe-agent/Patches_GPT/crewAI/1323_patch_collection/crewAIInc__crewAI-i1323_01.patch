diff --git a/reproduce_error.py b/reproduce_error.py
new file mode 100644
index 0000000..39301cc
--- /dev/null
+++ b/reproduce_error.py
@@ -0,0 +1,18 @@
+from crewai.llm import LLM
+
+def main():
+    # Define the LLM with the problematic model and parameters
+    llm = LLM(model="o1-preview", temperature=1, stop=["Test"])
+
+    # Prepare a simple message to send to the LLM
+    messages = "Hello, how are you?"
+
+    try:
+        # Call the LLM and print the response
+        response = llm._prepare_completion_params(messages=messages)
+        print("LLM Parameters:", response)
+    except Exception as e:
+        print("Error:", e)
+
+if __name__ == "__main__":
+    main()
\ No newline at end of file
diff --git a/src/crewai/llm.py b/src/crewai/llm.py
index 7415446..3d1fbc5 100644
--- a/src/crewai/llm.py
+++ b/src/crewai/llm.py
@@ -325,7 +325,7 @@ class LLM(BaseLLM):
             "temperature": self.temperature,
             "top_p": self.top_p,
             "n": self.n,
-            "stop": self.stop,
+            "stop": self.stop if self.model not in ["o1-preview", "o1-mini"] else None,
             "max_tokens": self.max_tokens or self.max_completion_tokens,
             "presence_penalty": self.presence_penalty,
             "frequency_penalty": self.frequency_penalty,
