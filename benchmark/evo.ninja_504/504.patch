From c752249a45cb41a727555ff5c896a308fefa5a3e Mon Sep 17 00:00:00 2001
From: dOrgJelli <jelli@dorg.tech>
Date: Thu, 30 Nov 2023 15:51:32 +0100
Subject: [PATCH] fix: properly chunk embedding input before passing to the
 proxy api

---
 apps/browser/src/api/ProxyEmbeddingApi.ts | 91 +++++++++++++----------
 1 file changed, 50 insertions(+), 41 deletions(-)

diff --git a/apps/browser/src/api/ProxyEmbeddingApi.ts b/apps/browser/src/api/ProxyEmbeddingApi.ts
index 14a4f987..da07d9d4 100644
--- a/apps/browser/src/api/ProxyEmbeddingApi.ts
+++ b/apps/browser/src/api/ProxyEmbeddingApi.ts
@@ -13,8 +13,13 @@ export class ProxyEmbeddingApi implements EmbeddingApi {
   private _goalId: string | undefined = undefined;
 
   constructor(
-    private _tokenizer: Tokenizer,
-    private _setCapReached: () => void
+    private tokenizer: Tokenizer,
+    private _setCapReached: () => void,
+    private modelConfig: {
+      model: string,
+      maxTokensPerInput: number,
+      maxInputsPerRequest: number
+    } = DEFAULT_ADA_CONFIG,
   ) {}
 
   public setGoalId(goalId: string | undefined) {
@@ -25,54 +30,58 @@ export class ProxyEmbeddingApi implements EmbeddingApi {
     input: string | string[],
     tries?: number
   ): Promise<EmbeddingCreationResult[]> {
-    const config = DEFAULT_ADA_CONFIG;
-
-    const inputs = Array.isArray(input) ? input : [input];
-    for (const input of inputs) {
-      if (this._tokenizer.encode(input).length > config.maxTokensPerInput) {
-        throw new Error(
-          `Input exceeds max request tokens: ${config.maxTokensPerInput}`
-        );
-      }
-    }
-
-    const batchedInputs = splitArray(inputs, config.maxInputsPerRequest);
-
     const goalId = this._goalId;
     if (!goalId) {
       throw Error("GoalID is not set");
     }
 
-    const embeddingResponse = await fetch("/api/proxy/embeddings", {
-      method: "POST",
-      body: JSON.stringify({
-        input: batchedInputs,
-        model: config.model,
-        goalId
-      }),
-      headers: {
-        "Content-Type": "application/json",
-      },
-    });
+    const inputs = Array.isArray(input) ? input : [input];
+    this.validateInput(inputs);
 
-    if (!embeddingResponse.ok) {
-      const error = await embeddingResponse.json();
-      if (embeddingResponse.status === 400) {
-        throw Error("Error from OpenAI Embeddings: " + error.error);
-      }
-      if (embeddingResponse.status === 429) {
-        await new Promise((resolve) => setTimeout(resolve, 15000));
-        if (!tries || tries < MAX_RATE_LIMIT_RETRIES) {
-          return this.createEmbeddings(input, tries == undefined ? 0 : ++tries);
+    const batchedInputs = splitArray(inputs, this.modelConfig.maxInputsPerRequest);
+
+    const results = await Promise.all(batchedInputs.map(async (inputs) => {
+      const embeddingResponse = await fetch("/api/proxy/embeddings", {
+        method: "POST",
+        body: JSON.stringify({
+          input: inputs,
+          model: this.modelConfig.model,
+          goalId
+        }),
+        headers: {
+          "Content-Type": "application/json",
+        },
+      });
+
+      if (!embeddingResponse.ok) {
+        const error = await embeddingResponse.json();
+        if (embeddingResponse.status === 400) {
+          throw Error("Error from OpenAI Embeddings: " + error.error);
+        }
+        if (embeddingResponse.status === 429) {
+          await new Promise((resolve) => setTimeout(resolve, 15000));
+          if (!tries || tries < MAX_RATE_LIMIT_RETRIES) {
+            return this.createEmbeddings(input, tries == undefined ? 0 : ++tries);
+          }
+        }
+        if (embeddingResponse.status === 403) {
+          this._setCapReached();
+          throw Error(ERROR_SUBSIDY_MAX);
         }
+        throw Error("Error trying to get response from embeddings proxy.", error);
       }
-      if (embeddingResponse.status === 403) {
-        this._setCapReached();
-        throw Error(ERROR_SUBSIDY_MAX);
+      const { embeddings } = await embeddingResponse.json();
+      return embeddings;
+    }))
+
+    return results.flat();
+  }
+
+  validateInput(inputs: string[]) {
+    for (const input of inputs) {
+      if (this.tokenizer.encode(input).length > this.modelConfig.maxTokensPerInput) {
+        throw new Error(`Input exceeds max request tokens: ${this.modelConfig.maxTokensPerInput}`);
       }
-      throw Error("Error trying to get response from embeddings proxy.", error);
     }
-    const { embeddings } = await embeddingResponse.json();
-    return embeddings;
   }
 }